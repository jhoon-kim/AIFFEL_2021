{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.뉴스 요약봇 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.9/site-packages (1.1.0)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (1.7.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (1.21.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (2.26.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (7.28.0)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.9/site-packages (from keras-tuner) (2.7.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (3.0.22)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (59.4.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->keras-tuner) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->keras-tuner) (2.0.8)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (2.3.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (3.19.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (2.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect>4.3->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "뉴스 요약에 사용하고자 하는 [데이터](https://github.com/sunnysai12345/News_Summary)는 2017년 2월부터 8월 사이에 `The Hindu`, `Indiantimes`와 `Guardian`지에 실린 기사를 수집한 것입니다.  \n",
    "데이터의 출처가 인도 언론임을 감안하고 데이터를 사용해야 할 것으로 보입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\",\n",
    "    filename=\"news_summary_more.csv\",\n",
    ")\n",
    "data = pd.read_csv(\"news_summary_more.csv\", encoding=\"iso-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98401\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85878</th>\n",
       "      <td>CEATÃ¢ÂÂs new witty videos can help you save...</td>\n",
       "      <td>CEAT Tyres has released a new range of light-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9189</th>\n",
       "      <td>RBI board's decisions will be positive for eco...</td>\n",
       "      <td>Kotak Mahindra Bank's MD and CEO Uday Kotak we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77495</th>\n",
       "      <td>Wells Fargo cuts 69 top executive jobs post ac...</td>\n",
       "      <td>US bank Wells Fargo, recently hit by a fraudul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51516</th>\n",
       "      <td>In the end, truth wins: Delhi CM on 'office of...</td>\n",
       "      <td>Delhi Chief Minister Arvind Kejriwal on Friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6147</th>\n",
       "      <td>Made my peace with her: Deepika on Katrina att...</td>\n",
       "      <td>Talking about her relationship with Katrina Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56751</th>\n",
       "      <td>Do not enjoy dealing with Pakistan: US State Secy</td>\n",
       "      <td>US State Secretary Rex Tillerson has said that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52254</th>\n",
       "      <td>FBI once sent letter to Martin Luther urging h...</td>\n",
       "      <td>American civil rights activist Martin Luther K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84847</th>\n",
       "      <td>Waves hit journo reporting about 'destructive'...</td>\n",
       "      <td>A Kerala journalist was hit by waves from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60802</th>\n",
       "      <td>Pakistan fired 500 missiles in our territory: ...</td>\n",
       "      <td>Afghanistan on Wednesday said Pakistan fired o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72120</th>\n",
       "      <td>CPI-M chooses CM of 19 years as candidate for ...</td>\n",
       "      <td>The Communist Party of India-Marxist (CPI-M) h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "85878  CEATÃ¢ÂÂs new witty videos can help you save...   \n",
       "9189   RBI board's decisions will be positive for eco...   \n",
       "77495  Wells Fargo cuts 69 top executive jobs post ac...   \n",
       "51516  In the end, truth wins: Delhi CM on 'office of...   \n",
       "6147   Made my peace with her: Deepika on Katrina att...   \n",
       "56751  Do not enjoy dealing with Pakistan: US State Secy   \n",
       "52254  FBI once sent letter to Martin Luther urging h...   \n",
       "84847  Waves hit journo reporting about 'destructive'...   \n",
       "60802  Pakistan fired 500 missiles in our territory: ...   \n",
       "72120  CPI-M chooses CM of 19 years as candidate for ...   \n",
       "\n",
       "                                                    text  \n",
       "85878  CEAT Tyres has released a new range of light-h...  \n",
       "9189   Kotak Mahindra Bank's MD and CEO Uday Kotak we...  \n",
       "77495  US bank Wells Fargo, recently hit by a fraudul...  \n",
       "51516  Delhi Chief Minister Arvind Kejriwal on Friday...  \n",
       "6147   Talking about her relationship with Katrina Ka...  \n",
       "56751  US State Secretary Rex Tillerson has said that...  \n",
       "52254  American civil rights activist Martin Luther K...  \n",
       "84847  A Kerala journalist was hit by waves from the ...  \n",
       "60802  Afghanistan on Wednesday said Pakistan fired o...  \n",
       "72120  The Communist Party of India-Marxist (CPI-M) h...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "data.sample(10, random_state=2049)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터는 총 98401개의 헤드라인과 본문으로 구성되어 있습니다.  \n",
    "인코딩 문제로 정상적으로 출력되지 않는 글자들이 보입니다. 또한 문장 부호와 숫자들이 존재합니다.  \n",
    "해당 데이터에 대해 추후 전처리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabu, who played dark characters in films like 'Maachis', 'Haider' and 'AndhaDhun', said, \"Playing dark characters has become a thing now for female actors. When I was doing it, no one else was.\" \"It was not that I was trying to break the glass ceiling. Ye sab bas hota gaya and female characters ke liye base banta gaya,\" she added.\n",
      "\n",
      "Following Atal Bihari Vajpayee's demise, Sachin Tendulkar tweeted, \"India is at a great loss today.Shri #AtalBihariVajpayee ji's contributions to our nation have been innumerable.\" \"Asaman ko choo gaya, jo asmaan sa vishal tha, dharti mein simat gaya, jo mitti jaisa narm tha. Kaun hai jo Atal reh paya zindagi bhar, Atal banke wo zindagi ko paa gaya,\" Virender Sehwag tweeted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1188, 21325]:  # 1193\n",
    "    print(data[\"text\"][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 단어들이 영어로 되어있지만, 문장이나 발언을 인용하는 경우 힌디어와 영어가 합성된 \"[Hinglish](https://en.wikipedia.org/wiki/Hinglish)\"가 발견됩니다.\n",
    "해당 언어에 대한 불용어 처리도 필요하다고 판단됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98280\n",
      "98360\n"
     ]
    }
   ],
   "source": [
    "print(data[\"headlines\"].nunique())\n",
    "print(data[\"text\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "헤드라인에 대해서는 중복이 존재할 수 있으나, 본문에 대한 중복은 피하는 편이 좋으니 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98360\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋에 null 샘플은 존재하지 않아 별도의 처리가 필요하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 의미를 나타내는 단어들을 동일한 형태로 변환시켜주는 텍스트 정규화를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어는 일반적으로 자연어 처리에 도움이 되지 않습니다. nltk에서 제공하는 정보를 사용하여 전처리 과정에서 불용어 제거를 적용합니다.  \n",
    "앞서 확인한 바와 같이 Hinglish에 대한 불용어 처리도 준비합니다. [(Hinglish 불용어 출처)](https://github.com/TrigonaMinima/HinglishNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once']\n",
      "\n",
      "Hinglish 불용어 개수: 1036\n",
      "['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani']\n",
      "\n",
      "합쳐진 불용어 개수: 1038\n",
      "['new', 'tu', 'inse', 'onto', 'through', 'oh', 'hereby', 'karenge', 'after', 'si', 'waala', 'k', 'to', 'kiska', \"couldn't\", 'o', 'kiski', 'becoming', 'such', 'keh', 'rather', 'where', 'di', 'my', \"weren't\", 'certainly', 'jinhi', 'several', 'went', 'hun', 'yahin', 'an', 'abe', 'inward', 'maane', 'ne', 'jyada', 'anyhow', 'iske', 'yourselves', 'kehte', 'toh', 'good', 'took', \"we'd\", 'should', 'kisko', 'was', 'kitni', 'mil', 'kino', 'causes', 'saara', 'nearly', 'seeing', 'none', 'using', 'waisa', 'both', 'ask', 'whereafter', \"didn't\", 'just', 'dunga', 'magar', \"ain't\", 'come', 'karega', 'other', 'secondly', \"where's\", 'honi', 'kam', 'kahan', 'later', 'therefore', 'doesn', 'nevertheless', 'get', 'between', 'kyu', 'via', 'first', 'however', 'rahe', 'up', 'inhe', 'below', 'bad', 'karu', 'hopefully', 'taken', \"hasn't\", 'upar', \"doesn't\", 'ex', \"i'll\", 'keeps', 'de', 'aisa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "print(\"영어 불용어 개수 :\", len(stopwords.words(\"english\")))\n",
    "print(stopwords.words(\"english\")[:100])\n",
    "print()\n",
    "\n",
    "with open(\"hingl.txt\") as f:\n",
    "    hinglish = f.read().splitlines()\n",
    "print(\"Hinglish 불용어 개수:\", len(hinglish))\n",
    "print(hinglish[:100])\n",
    "print()\n",
    "\n",
    "stopwords_ext = list(set(stopwords.words(\"english\") + hinglish))\n",
    "print(\"합쳐진 불용어 개수:\", len(stopwords_ext))\n",
    "print(stopwords_ext[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리를 수행하는 함수를 선언합니다.\n",
    "1. 모든 문자를 소문자로 치환\n",
    "2. 괄호로 감싸진 문자열 제거\n",
    "3. 큰따옴표 제거\n",
    "4. 축약어 제거\n",
    "5. 소유격 제거\n",
    "6. 영어 외 문자 공백으로 치환\n",
    "7. 불용어 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"\\([^)]*\\)\", \"\", sentence)\n",
    "    sentence = re.sub('\"', \"\", sentence)\n",
    "    sentence = \" \".join(\n",
    "        [contractions[t] if t in contractions else t for t in sentence.split(\" \")]\n",
    "    )\n",
    "    sentence = re.sub(r\"'s\\b\", \"\", sentence)\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = \" \".join(\n",
    "            word\n",
    "            for word in sentence.split()\n",
    "            if not word in stopwords_ext\n",
    "            if len(word) > 1\n",
    "        )\n",
    "    else:\n",
    "        tokens = \" \".join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 함수를 테스트합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text (orig)    : A London zoo is offering people to name a cockroach after their exes on Valentine's Day for ÃÂ£1.50 (nearly Ã¢ÂÂ¹140). \"For those that don't quite require revenge, there's another way to make you feel better about getting back at your ex,\" the zoo said in a statement. The names will appear on zoo's 'roach board' on February 14. \n",
      "text           : london zoo offering cockroach exes valentine day require revenge make feel back zoo statement names zoo roach board february\n",
      "headline (orig): UK zoo offers people to name cockroach after their ex on Valentine's\n",
      "headline       : uk zoo offers people to name cockroach after their ex on valentine\n"
     ]
    }
   ],
   "source": [
    "idx = 26\n",
    "temp_text = data[\"text\"][idx]\n",
    "temp_headline = data[\"headlines\"][idx]\n",
    "\n",
    "print(\"text (orig)    :\", temp_text)\n",
    "print(\"text           :\", preprocess_sentence(temp_text))\n",
    "print(\"headline (orig):\", temp_headline)\n",
    "print(\"headline       :\", preprocess_sentence(temp_headline, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 함수에 선언되어 있는 내용과 같이 소문자 변환, 문장부호 제거 등이 적용되었음을 확인할 수 있습니다. 또한 앞서 확인했던 인코딩 문제로 정상적이지 않은 문자 역시 제거되었습니다.  \n",
    "요약에 대해서는 불용어 제거를 수행하지 않아 \"to\"와 같은 단어들이 남아있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 `text` 데이터에 대해서 전처리를 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 전처리 후 결과:  ['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers', 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy year pranav kaushik delhi techie bagged reward spending cred coins users cred coin rupee bill paid avail rewards brands ixigo bookmyshow ubereats cult fit', 'zealand defeated india wickets fourth odi hamilton thursday win match match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india seventh lowest total odi cricket history', 'aegon life iterm insurance plan customers enjoy tax benefits premiums paid save taxes plan life cover age years customers options insure critical illnesses disability accidental death benefit rider life cover age years', 'speaking sexual harassment allegations rajkumar hirani sonam kapoor hirani years metoo movement derailed metoo movement believe woman case reserve judgment added hirani accused assistant worked sanju']\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "for s in data[\"text\"]:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "print(\"text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`headlines` 데이터에 대해 전처리를 적용하지만 불용어 제거는 수행하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines 전처리 후 결과:  ['upgrad learner switches to career in ml al with salary hike', 'delhi techie wins free food from swiggy for one year on cred', 'new zealand end rohit sharma led india match winning streak', 'aegon life iterm insurance plan helps customers save tax', 'have known hirani for yrs what if metoo claims are not true sonam']\n"
     ]
    }
   ],
   "source": [
    "clean_summary = []\n",
    "for s in data[\"headlines\"]:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(\"headlines 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리의 결과로 문자열에 아무것도 없는 경우가 생길 수 있고, 비어있는 문자열이 존재하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"] = clean_text\n",
    "data[\"headlines\"] = clean_summary\n",
    "\n",
    "data.replace(\"\", np.nan, inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print(\"전체 샘플수 :\", (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최대 길이 제한을 정하기 위해 본문과 요약에 대한 통계를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 49\n",
      "텍스트의 평균 길이 : 30.859414396095975\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 16\n",
      "요약의 평균 길이 : 9.299532330215534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "text_len = [len(s.split()) for s in data[\"text\"]]\n",
    "summary_len = [len(s.split()) for s in data[\"headlines\"]]\n",
    "\n",
    "print(\"텍스트의 최소 길이 : {}\".format(np.min(text_len)))\n",
    "print(\"텍스트의 최대 길이 : {}\".format(np.max(text_len)))\n",
    "print(\"텍스트의 평균 길이 : {}\".format(np.mean(text_len)))\n",
    "print(\"요약의 최소 길이 : {}\".format(np.min(summary_len)))\n",
    "print(\"요약의 최대 길이 : {}\".format(np.max(summary_len)))\n",
    "print(\"요약의 평균 길이 : {}\".format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title(\"text\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title(\"headlines\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"text\")\n",
    "plt.hist(text_len, bins=40)\n",
    "plt.xlabel(\"length of samples\")\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"headlines\")\n",
    "plt.hist(summary_len, bins=40)\n",
    "plt.xlabel(\"length of samples\")\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트는 평균 길이가 30 이지만 값이 분포하는 범위가 넓은 것으로 확인됩니다.  \n",
    "텍스트 데이터의 최대 길이 제한은 49 보다 작은 40 으로 정합니다.  \n",
    "요약 데이터의 최대 길이 제한은 16 보다 작은 12 로 정합니다.  \n",
    "(각각은 약 80%의 비율로 임의로 설정한 수치입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 40\n",
    "summary_max_len = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지정한 최대 길이보다 짧은 데이터의 비율이 얼마나 되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 40 이하인 샘플의 비율: 0.9915209434729565\n",
      "전체 샘플 중 길이가 12 이하인 샘플의 비율: 0.9880337535583571\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if len(s.split()) <= max_len:\n",
    "            cnt = cnt + 1\n",
    "    print(\"전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s\" % (max_len, (cnt / len(nested_list))))\n",
    "\n",
    "\n",
    "below_threshold_len(text_max_len, data[\"text\"])\n",
    "below_threshold_len(summary_max_len, data[\"headlines\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 데이터는 길이가 40 이하인 샘플의 비율이 99.15%로 대부분의 샘플이 40 이하의 길이를 가지고 있습니다.  \n",
    "요약 데이터의 경우 길이가 12 이하인 샘플은 98.80%입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩 과정에서 시퀀스(문장)을 자르면 불완전한 문장이 될 가능성이 높기에, 길이 조건을 만족하지 못하는 문장 자체를 데이터에서 제외합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 96354\n"
     ]
    }
   ],
   "source": [
    "data = data[data[\"text\"].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data[\"headlines\"].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print(\"전체 샘플수 :\", (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요약 데이터에 문장 시작과 끝을 알려주는 토큰(`sostoken`, `eostoken`)을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>sostoken upgrad learner switches to career in ...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delhi techie wins free food from swiggy for on...</td>\n",
       "      <td>kunal shah credit card bill payment platform c...</td>\n",
       "      <td>sostoken delhi techie wins free food from swig...</td>\n",
       "      <td>delhi techie wins free food from swiggy for on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>zealand defeated india wickets fourth odi hami...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upgrad learner switches to career in ml al wit...   \n",
       "1  delhi techie wins free food from swiggy for on...   \n",
       "2  new zealand end rohit sharma led india match w...   \n",
       "3  aegon life iterm insurance plan helps customer...   \n",
       "5  rahat fateh ali khan denies getting notice for...   \n",
       "\n",
       "                                                text  \\\n",
       "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
       "1  kunal shah credit card bill payment platform c...   \n",
       "2  zealand defeated india wickets fourth odi hami...   \n",
       "3  aegon life iterm insurance plan customers enjo...   \n",
       "5  pakistani singer rahat fateh ali khan denied r...   \n",
       "\n",
       "                                       decoder_input  \\\n",
       "0  sostoken upgrad learner switches to career in ...   \n",
       "1  sostoken delhi techie wins free food from swig...   \n",
       "2  sostoken new zealand end rohit sharma led indi...   \n",
       "3  sostoken aegon life iterm insurance plan helps...   \n",
       "5  sostoken rahat fateh ali khan denies getting n...   \n",
       "\n",
       "                                      decoder_target  \n",
       "0  upgrad learner switches to career in ml al wit...  \n",
       "1  delhi techie wins free food from swiggy for on...  \n",
       "2  new zealand end rohit sharma led india match w...  \n",
       "3  aegon life iterm insurance plan helps customer...  \n",
       "5  rahat fateh ali khan denies getting notice for...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"decoder_input\"] = data[\"headlines\"].apply(lambda x: \"sostoken \" + x)\n",
    "data[\"decoder_target\"] = data[\"headlines\"].apply(lambda x: x + \" eostoken\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더에서 사용할 데이터와 디코더에서 사용할 데이터를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data[\"text\"])\n",
    "decoder_input = np.array(data[\"decoder_input\"])\n",
    "decoder_target = np.array(data[\"decoder_target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 데이터와 검증 데이터 분리\n",
    "검증 데이터 준비를 위해 훈련 데이터의 인덱스를 임의로 섞고, 20%를 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87613 17800 31548 ...  7693 44779 18762]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 77084\n",
      "훈련 레이블의 개수 : 77084\n",
      "테스트 데이터의 개수 : 19270\n",
      "테스트 레이블의 개수 : 19270\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input) * 0.2)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(\"훈련 데이터의 개수 :\", len(encoder_input_train))\n",
    "print(\"훈련 레이블의 개수 :\", len(decoder_input_train))\n",
    "print(\"테스트 데이터의 개수 :\", len(encoder_input_test))\n",
    "print(\"테스트 레이블의 개수 :\", len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수 인코딩\n",
    "준비된 훈련 데이터를 위한 단어 집합을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "src_tokenizer = Tokenizer()\n",
    "src_tokenizer.fit_on_texts(encoder_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도가 낮은 단어를 훈련 데이터에서 제외시키기 위해 각 단어의 등장 빈도를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 68188\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 46706\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기:  21482\n",
      "단어 집합에서 희귀 단어의 비율: 68.49592303631138\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.9887887341272394\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index)\n",
    "rare_cnt = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(\"단어 집합(vocabulary)의 크기 :\", total_cnt)\n",
    "print(f\"등장 빈도가 {threshold - 1}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
    "print(\"단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기: \", (total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt) * 100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "희귀 단어를 제외한 경우 단어 집합의 크기가 22,095이 되고, 근사한 값인 20,000으로 단어 집합의 크기를 제한합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 20000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab)\n",
    "src_tokenizer.fit_on_texts(encoder_input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크기가 제한된 단어 집합을 사용하여 텍스트 시퀀스를 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[794, 14245, 6943, 8391, 4266, 3785, 9372, 10504, 6943, 41, 79, 1141, 1168, 12038, 1, 1088, 1141, 755, 439, 18824, 84, 111, 1473, 3599, 83, 1141, 218, 1678, 18825, 91, 1, 11013, 3], [122, 11, 54, 11014, 1716, 7458, 2101, 5988, 1396, 6, 6744, 1269, 14246, 8251, 2342, 691, 402, 1397, 4441, 1716, 728, 2123, 2789, 33, 501, 3630, 2861, 2465, 652, 8671, 305, 1396, 383], [2962, 2041, 267, 100, 3529, 8856, 5479, 8392, 1063, 2188, 4671, 3529, 197, 1339, 147, 293, 315, 1, 320, 5479, 4198, 2789, 1353, 1689, 185, 137, 67]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 입력 데이터에 대해서도 희귀 단어를 확인하고 단어장의 크기를 결정하는 과정을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 29923\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 20521\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기:  9402\n",
      "단어 집합에서 희귀 단어의 비율: 68.57935367443105\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.445860114282245\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(tar_tokenizer.word_index)\n",
    "rare_cnt = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(\"단어 집합(vocabulary)의 크기 :\", total_cnt)\n",
    "print(f\"등장 빈도가 {threshold - 1}번 이하인 희귀 단어의 수: {rare_cnt}\")\n",
    "print(\"단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기: \", (total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt) * 100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 입력 데이터에 대해서는 단어 집합의 크기를 9,000으로 정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  [[1, 559, 2499, 5, 8, 6778, 890, 5569], [1, 963, 121, 3, 2500, 6403, 7, 2811, 4084], [1, 349, 563, 2168, 257, 25, 8676, 7148, 5817], [1, 105, 1225, 187, 6122, 3740, 9, 1315], [1, 279, 5149, 1685, 4948, 4, 347, 1015]]\n",
      "decoder  [[559, 2499, 5, 8, 6778, 890, 5569, 2], [963, 121, 3, 2500, 6403, 7, 2811, 4084, 2], [349, 563, 2168, 257, 25, 8676, 7148, 5817, 2], [105, 1225, 187, 6122, 3740, 9, 1315, 2], [279, 5149, 1685, 4948, 4, 347, 1015, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 9000\n",
    "\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab)\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "print(\"input \", decoder_input_train[:5])\n",
    "print(\"decoder \", decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도가 낮은 단어들을 제외시켰기 때문에, 해당 단어들로만 구성된 문장은 길이가 0이 되었을 것입니다.  \n",
    "따라서 길이가 0(`sostoken` 또는 `eostoken`을 제외)인 데이터를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 77084\n",
      "훈련 레이블의 개수 : 77084\n",
      "테스트 데이터의 개수 : 19270\n",
      "테스트 레이블의 개수 : 19270\n"
     ]
    }
   ],
   "source": [
    "drop_train = [\n",
    "    index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1\n",
    "]\n",
    "drop_test = [\n",
    "    index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1\n",
    "]\n",
    "\n",
    "print(\"삭제할 훈련 데이터의 개수 :\", len(drop_train))\n",
    "print(\"삭제할 테스트 데이터의 개수 :\", len(drop_test))\n",
    "\n",
    "encoder_input_train = [\n",
    "    sentence\n",
    "    for index, sentence in enumerate(encoder_input_train)\n",
    "    if index not in drop_train\n",
    "]\n",
    "decoder_input_train = [\n",
    "    sentence\n",
    "    for index, sentence in enumerate(decoder_input_train)\n",
    "    if index not in drop_train\n",
    "]\n",
    "decoder_target_train = [\n",
    "    sentence\n",
    "    for index, sentence in enumerate(decoder_target_train)\n",
    "    if index not in drop_train\n",
    "]\n",
    "\n",
    "encoder_input_test = [\n",
    "    sentence\n",
    "    for index, sentence in enumerate(encoder_input_test)\n",
    "    if index not in drop_test\n",
    "]\n",
    "decoder_input_test = [\n",
    "    sentence\n",
    "    for index, sentence in enumerate(decoder_input_test)\n",
    "    if index not in drop_test\n",
    "]\n",
    "decoder_target_test = [\n",
    "    sentence\n",
    "    for index, sentence in enumerate(decoder_target_test)\n",
    "    if index not in drop_test\n",
    "]\n",
    "\n",
    "print(\"훈련 데이터의 개수 :\", len(encoder_input_train))\n",
    "print(\"훈련 레이블의 개수 :\", len(decoder_input_train))\n",
    "print(\"테스트 데이터의 개수 :\", len(encoder_input_test))\n",
    "print(\"테스트 레이블의 개수 :\", len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시퀀스의 최대 길이를 제한하며, 길이가 부족한 시퀀스에 대해서는 패딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(\n",
    "    encoder_input_train, maxlen=text_max_len, padding=\"post\"\n",
    ")\n",
    "encoder_input_test = pad_sequences(\n",
    "    encoder_input_test, maxlen=text_max_len, padding=\"post\"\n",
    ")\n",
    "decoder_input_train = pad_sequences(\n",
    "    decoder_input_train, maxlen=summary_max_len, padding=\"post\"\n",
    ")\n",
    "decoder_target_train = pad_sequences(\n",
    "    decoder_target_train, maxlen=summary_max_len, padding=\"post\"\n",
    ")\n",
    "decoder_input_test = pad_sequences(\n",
    "    decoder_input_test, maxlen=summary_max_len, padding=\"post\"\n",
    ")\n",
    "decoder_target_test = pad_sequences(\n",
    "    decoder_target_test, maxlen=summary_max_len, padding=\"post\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계\n",
    "모델의 인코더를 설계합니다.  \n",
    "- seq2seq 구조에 attention을 추가한 구조를 사용하겠습니다.  \n",
    "- 인코더에서 LSTM은 3개의 층을 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    AdditiveAttention,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "encoder_lstm1 = LSTM(\n",
    "    hidden_size,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    dropout=0.4,\n",
    ")\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "encoder_lstm2 = LSTM(\n",
    "    hidden_size,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    dropout=0.4,\n",
    ")\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "encoder_lstm3 = LSTM(\n",
    "    hidden_size,\n",
    "    return_state=True,\n",
    "    return_sequences=True,\n",
    "    dropout=0.4,\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 디코더를 설계합니다.  \n",
    "- 성능 향상을 위해서 attention 기법을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 128)      2560000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 40, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 40, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1152000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 40, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9000)   4617000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,168,360\n",
      "Trainable params: 10,168,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(\n",
    "    hidden_size,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    dropout=0.4,\n",
    ")\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "attn_layer = AdditiveAttention(name=\"attention_layer\")\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_outputs, attn_out]\n",
    ")\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation=\"softmax\")\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "302/302 [==============================] - 34s 90ms/step - loss: 5.5325 - val_loss: 5.1157\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 26s 88ms/step - loss: 4.9627 - val_loss: 4.7924\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 27s 90ms/step - loss: 4.6200 - val_loss: 4.5193\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 27s 89ms/step - loss: 4.3543 - val_loss: 4.3321\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 27s 89ms/step - loss: 4.1500 - val_loss: 4.1938\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 27s 90ms/step - loss: 3.9830 - val_loss: 4.0980\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 27s 90ms/step - loss: 3.8398 - val_loss: 3.9919\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 27s 91ms/step - loss: 3.7141 - val_loss: 3.9111\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 27s 91ms/step - loss: 3.6048 - val_loss: 3.8587\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 27s 91ms/step - loss: 3.5068 - val_loss: 3.8099\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 28s 91ms/step - loss: 3.4189 - val_loss: 3.7689\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 28s 91ms/step - loss: 3.3369 - val_loss: 3.7416\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 28s 91ms/step - loss: 3.2630 - val_loss: 3.7069\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 28s 91ms/step - loss: 3.1960 - val_loss: 3.6903\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 28s 91ms/step - loss: 3.1332 - val_loss: 3.6717\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 28s 92ms/step - loss: 3.0761 - val_loss: 3.6621\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 29s 95ms/step - loss: 3.0211 - val_loss: 3.6466\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 28s 93ms/step - loss: 2.9694 - val_loss: 3.6517\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 28s 93ms/step - loss: 2.9197 - val_loss: 3.6290\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 28s 94ms/step - loss: 2.8751 - val_loss: 3.6216\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 28s 93ms/step - loss: 2.8336 - val_loss: 3.6123\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 28s 93ms/step - loss: 2.7932 - val_loss: 3.6112\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 28s 94ms/step - loss: 2.7555 - val_loss: 3.6098\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 28s 93ms/step - loss: 2.7195 - val_loss: 3.6106\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 28s 94ms/step - loss: 2.6857 - val_loss: 3.6150\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    x=[encoder_input_train, decoder_input_train],\n",
    "    y=decoder_target_train,\n",
    "    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "    batch_size=256,\n",
    "    callbacks=[es],\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 결과를 그래프로 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 50 epoch 중 25 epoch 이후 훈련을 중단하였습니다.(validation loss가 2 epoch 동안 감소하지 않음)    \n",
    "그래프를 보면 5 epoch 부근을 기준으로 학습이 진행되면서 validation loss와 train loss의 차이가 점점 벌어짐을 확인할 수 있습니다.(과적합 양상으로 판단됩니다.)  \n",
    "최종 validation loss는 3.62으로 첫 epoch 이후 얻었던 5.12과 비교하여 약 1.50 정도 감소하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 위해 아래의 3가지 항목을 준비합니다.\n",
    "1. 텍스트 원문 단어 집합을 참고하여 숫자를 단어로 변환하는 사전\n",
    "2. 요약문 단어 집합을 참고하여 단어를 숫자로 변환하는 사전\n",
    "3. 요약문 단어 집합을 참고하여 숫자를 단어로 변환하는 사전\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word\n",
    "tar_word_to_index = tar_tokenizer.word_index\n",
    "tar_index_to_word = tar_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 위한 인코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(\n",
    "    inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 위한 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "    dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "    [decoder_outputs2, attn_out_inf]\n",
    ")\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs]\n",
    "    + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 테스트를 위해 디코딩 함수를 작성합니다.  \n",
    "- `target_seq`에 `sostoken`에 해당하는 토큰을 입력합니다.\n",
    "- 입력 데이터에 대한 모델의 출력 결과를 모델에 다시 입력으로 사용하여 문장을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tar_word_to_index[\"sostoken\"]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if sampled_token != \"eostoken\":\n",
    "            decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"eostoken\" or len(decoded_sentence.split()) >= (\n",
    "            summary_max_len - 1\n",
    "        ):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 결과와 요약문 비교하기\n",
    "모델을 통해서 얻은 결과와 실제 데이터를 비교하기 위한 함수를 작성합니다.\n",
    "1. (원문) 정수로 표현된 데이터를 텍스트로 변환하는 함수\n",
    "2. (요약) 정수로 표현된 데이터를 요약문으로 변환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2text(input_seq):\n",
    "    temp = \"\"\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            temp = temp + src_index_to_word[i] + \" \"\n",
    "    return temp\n",
    "\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    temp = \"\"\n",
    "    for i in input_seq:\n",
    "        if (i != 0 and i != tar_word_to_index[\"sostoken\"]) and i != tar_word_to_index[\n",
    "            \"eostoken\"\n",
    "        ]:\n",
    "            temp = temp + tar_index_to_word[i] + \" \"\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문      : comedian turned actor andy dick fired movies accused sexual misconduct harassment sets andy accused groping consensual kissing kissed cheek bye funny \n",
      "실제 요약 : comedian andy fired from films post sexual misconduct claims \n",
      "예측 요약 :  am not my friend weinstein on sexual harassment\n",
      "\n",
      "\n",
      "원문      : janhvi kapoor talking school days school days make stories remember calling friend secret agent spy school believe added \n",
      "실제 요약 : in school days people thought was janhvi \n",
      "예측 요약 :  have been to be for the class of kids janhvi\n",
      "\n",
      "\n",
      "원문      : england ended day fourth test innings trailing india runs india cheteshwar pujara scored maiden test ton england pujara scored registered run run stand ninth tenth wickets respectively england moeen ali picked indian wickets runs \n",
      "실제 요약 : england trail by runs after pujara scores st ton in eng \n",
      "예측 요약 :  pujara end day as england end day at home\n",
      "\n",
      "\n",
      "원문      : virat kohli playing test captain south africa johannesburg fielded different playing xi tests kohli different players matches ravichandran ashwin featuring tests player kohli captaincy kohli captaincy \n",
      "실제 요약 : india has played unique teams in all tests under kohli \n",
      "예측 요약 :  kohli will lead to play in test series kohli\n",
      "\n",
      "\n",
      "원문      : north korea estimated nuclear weapons south korea unification minister cho informed country parliament cited information intelligence authorities south korea maintained accept north korea nuclear state continue achieve denuclearisation korean peninsula \n",
      "실제 요약 : korea to have up to nuclear weapons korea \n",
      "예측 요약 :  north korea to be world most powerful north korea\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(78, 83):\n",
    "    print(\"원문      :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation loss가 3.62인 모델의 출력 결과는 요약문에 포함되어 있는 핵심 단어를 포함하는 경우도 있었으나, 전하고자 하는 내용을 전달하지 못하는 경우가 많았습니다.  \n",
    "더 낮은 validation loss를 달성한 모델의 요약 결과는 어떤지 확인하기 위해 모델 탐색을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import IPython\n",
    "\n",
    "\n",
    "trials = 24\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    n_layers = hp.Choice(\"n_layers\", [1, 2, 3])\n",
    "    embedding_dim = hp.Choice(\"embedding_dim\", [128, 256])\n",
    "    hidden_size = hp.Choice(\"hidden_size\", [256, 512])\n",
    "    optim = hp.Choice(\"optim\", [\"rmsprop\", \"adam\"])\n",
    "\n",
    "    encoder_inputs = Input(shape=(text_max_len,))\n",
    "    enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "    if n_layers >= 1:\n",
    "        encoder_lstm1 = LSTM(\n",
    "            hidden_size, return_sequences=True, return_state=True, dropout=0.4\n",
    "        )\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm1(enc_emb)\n",
    "    if n_layers >= 2:\n",
    "        encoder_lstm2 = LSTM(\n",
    "            hidden_size, return_sequences=True, return_state=True, dropout=0.4\n",
    "        )\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
    "    if n_layers >= 3:\n",
    "        encoder_lstm3 = LSTM(\n",
    "            hidden_size, return_state=True, return_sequences=True, dropout=0.4\n",
    "        )\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm3(encoder_outputs)\n",
    "\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    decoder_lstm = LSTM(\n",
    "        hidden_size, return_sequences=True, return_state=True, dropout=0.4\n",
    "    )\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "    attn_layer = AdditiveAttention(name=\"attention_layer\")\n",
    "    attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "    decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "        [decoder_outputs, attn_out]\n",
    "    )\n",
    "    decoder_softmax_layer = Dense(tar_vocab, activation=\"softmax\")\n",
    "    decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "    model.compile(optimizer=optim, loss=\"sparse_categorical_crossentropy\")\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    model_builder,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=trials,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"log_exp\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 훈련한 모델이 5 epoch 부근에서 validation loss가 train loss보다 커지기 시작하였기에, 5 epoch 동안 훈련한 성능을 기준으로 최적의 모델을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 Complete [00h 02m 12s]\n",
      "val_loss: 3.9114990234375\n",
      "\n",
      "Best val_loss So Far: 3.8376500606536865\n",
      "Total elapsed time: 01h 02m 03s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete.\n",
      "n_layer: 1\n",
      "embedding_dim: 256\n",
      "hidden_size: 512\n",
      "optim: adam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    x=[encoder_input_train, decoder_input_train],\n",
    "    y=decoder_target_train,\n",
    "    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "    callbacks=[ClearTrainingOutput()],\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    ")\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "n_layer: {best_hps.get('n_layers')}\n",
    "embedding_dim: {best_hps.get('embedding_dim')}\n",
    "hidden_size: {best_hps.get('hidden_size')}\n",
    "optim: {best_hps.get('optim')}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "탐색 결과 LSTM은 1개의 계층을 사용하고, 임베딩 크기와 히든 유닛은 각각 256, 512로 설정한 모델의 validation loss가 가장 낮았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 모델 훈련\n",
    "앞서 탐색한 하이퍼파라미터를 적용한 모델을 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "302/302 [==============================] - 44s 134ms/step - loss: 5.6064 - val_loss: 5.2544\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 5.1251 - val_loss: 4.8751\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 4.6064 - val_loss: 4.3588\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 4.0972 - val_loss: 3.9812\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 3.6712 - val_loss: 3.7257\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 3.3096 - val_loss: 3.5638\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 3.0041 - val_loss: 3.4639\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 2.7366 - val_loss: 3.4118\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 2.5004 - val_loss: 3.3904\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 2.2909 - val_loss: 3.3910\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 40s 133ms/step - loss: 2.1071 - val_loss: 3.4009\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "n_layers = best_hps.get(\"n_layers\")\n",
    "embedding_dim = best_hps.get(\"embedding_dim\")\n",
    "hidden_size = best_hps.get(\"hidden_size\")\n",
    "optim = best_hps.get(\"optim\")\n",
    "\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "if n_layers >= 1:\n",
    "    encoder_lstm1 = LSTM(\n",
    "        hidden_size, return_sequences=True, return_state=True, dropout=0.4\n",
    "    )\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm1(enc_emb)\n",
    "if n_layers >= 2:\n",
    "    encoder_lstm2 = LSTM(\n",
    "        hidden_size, return_sequences=True, return_state=True, dropout=0.4\n",
    "    )\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
    "if n_layers >= 3:\n",
    "    encoder_lstm3 = LSTM(\n",
    "        hidden_size, return_state=True, return_sequences=True, dropout=0.4\n",
    "    )\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm3(encoder_outputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "attn_layer = AdditiveAttention(name=\"attention_layer\")\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_outputs, attn_out]\n",
    ")\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation=\"softmax\")\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "\n",
    "model.compile(optimizer=optim, loss=\"sparse_categorical_crossentropy\")\n",
    "history = model.fit(\n",
    "    x=[encoder_input_train, decoder_input_train],\n",
    "    y=decoder_target_train,\n",
    "    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "    batch_size=256,\n",
    "    callbacks=[es],\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 epoch으로 훈련이 종료되었으며, 3.40의 validation loss를 달성하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트를 위한 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(\n",
    "    inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c]\n",
    ")\n",
    "\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "    dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "    [decoder_outputs2, attn_out_inf]\n",
    ")\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs]\n",
    "    + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 모델 예측 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문      : comedian turned actor andy dick fired movies accused sexual misconduct harassment sets andy accused groping consensual kissing kissed cheek bye funny \n",
      "실제 요약 : comedian andy fired from films post sexual misconduct claims \n",
      "예측 요약 :  comedian fired for sexual assault allegation\n",
      "\n",
      "\n",
      "원문      : janhvi kapoor talking school days school days make stories remember calling friend secret agent spy school believe added \n",
      "실제 요약 : in school days people thought was janhvi \n",
      "예측 요약 :  am not going to be called for kids janhvi on soty\n",
      "\n",
      "\n",
      "원문      : england ended day fourth test innings trailing india runs india cheteshwar pujara scored maiden test ton england pujara scored registered run run stand ninth tenth wickets respectively england moeen ali picked indian wickets runs \n",
      "실제 요약 : england trail by runs after pujara scores st ton in eng \n",
      "예측 요약 :  pujara pujara ton on runs in australia end day\n",
      "\n",
      "\n",
      "원문      : virat kohli playing test captain south africa johannesburg fielded different playing xi tests kohli different players matches ravichandran ashwin featuring tests player kohli captaincy kohli captaincy \n",
      "실제 요약 : india has played unique teams in all tests under kohli \n",
      "예측 요약 :  kohli st captain to be catches in test series\n",
      "\n",
      "\n",
      "원문      : north korea estimated nuclear weapons south korea unification minister cho informed country parliament cited information intelligence authorities south korea maintained accept north korea nuclear state continue achieve denuclearisation korean peninsula \n",
      "실제 요약 : korea to have up to nuclear weapons korea \n",
      "예측 요약 :  korea sanctions against korea over nuke talks\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(78, 83):\n",
    "    print(\"원문      :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플로 확인한 데이터에 대해서 실제 요약에 근접한 결과는 아니지만, validation loss가 3.62인 모델과 비교하여 실제 요약에 포함되어 있는 단어를 예측하는 비율이 높아졌습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summa을 이용한 추출적 요약\n",
    "[Summa](https://github.com/summanlp/textrank) 라이브러리를 사용하여 동일한 데이터에 대해 추출적 요약을 시도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"news_summary_more.csv\", encoding=\"iso-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text    : North Korea is \"unlikely to completely give up its nuclear weapons and production capabilities\", Director of US' National Intelligence, Daniel Coats, said. \"North Korean leaders view nuclear arms as critical to regime survival,\" he added. His statements come despite North Korea's agreement with the US at the Singapore summit last year to denuclearise the Korean Peninsula. \n",
      "Headline: N Korea unlikely to give up nuclear weapons: US intelligence chief\n",
      "Summa   : \"North Korean leaders view nuclear arms as critical to regime survival,\" he added.\n",
      "\n",
      "Text    : Jet Airways on Wednesday denied it was directed by GE Capital Aviation Services to ground its aircraft due to non-payment of dues. The airline said it's in the process of delivering three aircraft to lessors due to the expiry of lease terms. Further, three aircraft have been temporarily grounded to carry out an engine normalisation exercise, it added.\n",
      "Headline: Jet Airways denies being told to ground aircraft\n",
      "Summa   : Further, three aircraft have been temporarily grounded to carry out an engine normalisation exercise, it added.\n",
      "\n",
      "Text    : Retired Justice Srikrishna's enquiry panel has found that ex-CEO Chanda Kochhar violated ICICI Bank's code of conduct. The bank said it'll treat Kochhar's exit as termination and will take back all bonuses paid to her from April 2009 until March 2018. The CBI recently filed a case against Kochhar, her husband, and Videocon MD in the ICICI-Videocon loan case.\n",
      "Headline: ICICI fires Chanda Kochhar, to take back bonuses paid in last 9 yrs\n",
      "Summa   : Retired Justice Srikrishna's enquiry panel has found that ex-CEO Chanda Kochhar violated ICICI Bank's code of conduct.\n",
      "\n",
      "Text    : Ex-India cricketer Jacob Martin has been shifted to general ward from the ICU after being on a ventilator for nearly a month following a road accident, wherein he severely injured his lungs. Martin's wife Khyati mentioned there are chances that Jacob will be in hospital for two more months. BCCI, CSK and various cricketers provided financial assistance for Martin's treatment.\n",
      "Headline: Ex-cricketer Martin out of ICU after being on ventilator for a month\n",
      "Summa   : Ex-India cricketer Jacob Martin has been shifted to general ward from the ICU after being on a ventilator for nearly a month following a road accident, wherein he severely injured his lungs.\n",
      "\n",
      "Text    : Sri Lanka all-rounder Thisara Perera has written to Sri Lanka Cricket CEO Ashley de Silva, asking the board to intervene after ODI captain Lasith Malinga's wife Tanya targeted him on Facebook. Tanya accused Perera of meeting country's Sports Minister to secure his place in the team. \"We've become laughing stock of whole country...because of one person's personal vendetta,\" Perera wrote.\n",
      "Headline: Malinga's wife targets Perera on Facebook, he seeks board's help\n",
      "Summa   : Tanya accused Perera of meeting country's Sports Minister to secure his place in the team.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(78, 83):\n",
    "    print(\"Text    :\", data[\"text\"][i])\n",
    "    print(\"Headline:\", data[\"headlines\"][i])\n",
    "    print(\"Summa   :\", summarize(data[\"text\"][i], ratio=0.5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text    : North Korea is \"unlikely to completely give up its nuclear weapons and production capabilities\", Director of US' National Intelligence, Daniel Coats, said. \"North Korean leaders view nuclear arms as critical to regime survival,\" he added. His statements come despite North Korea's agreement with the US at the Singapore summit last year to denuclearise the Korean Peninsula. \n",
      "Headline: N Korea unlikely to give up nuclear weapons: US intelligence chief\n",
      "Summa   : \"North Korean leaders view nuclear arms as critical to regime survival,\" he added.\n",
      "\n",
      "Text    : Jet Airways on Wednesday denied it was directed by GE Capital Aviation Services to ground its aircraft due to non-payment of dues. The airline said it's in the process of delivering three aircraft to lessors due to the expiry of lease terms. Further, three aircraft have been temporarily grounded to carry out an engine normalisation exercise, it added.\n",
      "Headline: Jet Airways denies being told to ground aircraft\n",
      "Summa   : Further, three aircraft have been temporarily grounded to carry out an engine normalisation exercise, it added.\n",
      "\n",
      "Text    : Retired Justice Srikrishna's enquiry panel has found that ex-CEO Chanda Kochhar violated ICICI Bank's code of conduct. The bank said it'll treat Kochhar's exit as termination and will take back all bonuses paid to her from April 2009 until March 2018. The CBI recently filed a case against Kochhar, her husband, and Videocon MD in the ICICI-Videocon loan case.\n",
      "Headline: ICICI fires Chanda Kochhar, to take back bonuses paid in last 9 yrs\n",
      "Summa   : Retired Justice Srikrishna's enquiry panel has found that ex-CEO Chanda Kochhar violated ICICI Bank's code of conduct.\n",
      "\n",
      "Text    : Ex-India cricketer Jacob Martin has been shifted to general ward from the ICU after being on a ventilator for nearly a month following a road accident, wherein he severely injured his lungs. Martin's wife Khyati mentioned there are chances that Jacob will be in hospital for two more months. BCCI, CSK and various cricketers provided financial assistance for Martin's treatment.\n",
      "Headline: Ex-cricketer Martin out of ICU after being on ventilator for a month\n",
      "Summa   : \n",
      "\n",
      "Text    : Sri Lanka all-rounder Thisara Perera has written to Sri Lanka Cricket CEO Ashley de Silva, asking the board to intervene after ODI captain Lasith Malinga's wife Tanya targeted him on Facebook. Tanya accused Perera of meeting country's Sports Minister to secure his place in the team. \"We've become laughing stock of whole country...because of one person's personal vendetta,\" Perera wrote.\n",
      "Headline: Malinga's wife targets Perera on Facebook, he seeks board's help\n",
      "Summa   : Tanya accused Perera of meeting country's Sports Minister to secure his place in the team.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(78, 83):\n",
    "    print(\"Text    :\", data[\"text\"][i])\n",
    "    print(\"Headline:\", data[\"headlines\"][i])\n",
    "    print(\"Summa   :\", summarize(data[\"text\"][i], words=10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "### Abstractive 요약 모델\n",
    "\n",
    "- 영어를 기준으로 불용어를 제거하였기에, 데이터셋의 특성을 제대로 반영하지 못하여 모델 훈련에 좋지 않은 영향을 미쳤을 것으로 생각됩니다.\n",
    "- validation loss가 최종 3.40으로 요약 모델이 성공적으로 잘 훈련되었는가에 대해서는 추가적인 고찰이 필요합니다.\n",
    "    - 문장의 요약은 특정한 기준이 존재하는 것이 아니라 데이터셋을 제작한 사람 개인의 주관이 들어가 있어, 절대적인 정답이라고 말할 수 없습니다.\n",
    "    - 또한 모델이 요약한 결과에 대한 평가 역시 개개인에 따라 달라질 수 있습니다. 따라서 다양한 사람들의 평가를 통해 적절한 훈련 목표치를 설정하는 것이 필요합니다.\n",
    "- 사용한 데이터셋에 인도에서 사용되는 고유 명사로 추정되는 단어들이 많이 등장하여 요약 결과를 보고 성능을 평가하기에는 한계가 존재했습니다.\n",
    "    - 그럼에도 주관적인 판단으로 적절한 요약을 한 경우가 존재했습니다.\n",
    "    \n",
    "###  Abstractive 요약 결과와 Extractive 요약 결과 비교\n",
    "\n",
    "- abstractive 요약은 원문에서 등장하지 않았던 단어를 사용하는 경우도 존재하였습니다.\n",
    "- extractive 요약은 기사 속의 문장을 그대로 사용한 결과를 보여주었고, 단어를 조합하기보다는 원문에 존재하는 온전한 문장을 선택하는 경우가 많았습니다.\n",
    "    - 또한 임의로 제시한 50%의 비율과 10개의 단어 제한에 대해서 결과가 출력되지 않는 경우도 존재하였습니다.\n",
    "- 두 요약 결과 모두 원문의 핵심이라고 생각되는 단어들을 포함하는 결과를 얻을 수 있었습니다.\n",
    "- extractive 요약은 원문에 존재하는 문장을 그대로 사용하려는 경향 때문에 자연스러운 문장을 얻을 수 있었지만, abstractive 요약은 문장을 생성해냈기에 상대적으로 어색한 문장을 만들었습니다.\n",
    "- 소설이나 에세이와 다르게 객관적 사실만을 간결하게 작성하는 언론 기사의 특성상 전체적인 내용을 보고 요약하는 abstractive 모델 보다는 extractive 모델의 결과가 더 좋은 것으로 판단됩니다.\n",
    "    - 다만 훈련 데이터의 크기와 최적의 모델을 찾는 과정이 더 확장된 범위에서 진행된다면 조금 다른 결과를 얻을 수도 있을 것으로 생각됩니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "|평가문항|상세기준|\n",
    "|---|---|\n",
    "|1. Abstractive 모델 구성을 위한 텍스트 전처리 단계가 체계적으로 진행되었다.|분석단계, 정제단계, 정규화와 불용어 제거, 데이터셋 분리, 인코딩 과정이 빠짐없이 체계적으로 진행되었다.|\n",
    "|2. 텍스트 요약모델이 성공적으로 학습되었음을 확인하였다.|모델학습이 안정적으로 수렴되었음을 그래프를 통해 확인하였으며, 실제 요약문과 유사한 요약문장을 얻을 수 있었다.|\n",
    "|3. Extractive 요약을 시도해 보고 Abstractive 요약 결과과 함께 비교해 보았다.|두 요약 결과를 문법완성도 측면과 핵심단어 포함 측면으로 나누어 비교분석 결과를 제시하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 루브릭 자체평가\n",
    "1. 데이터를 확인하고 전처리 함수를 적용하였다.\n",
    "2. 요약 모델의 학습이 정상적으로 이루어졌으며, 모델을 통하여 요약 문장을 얻을 수 있었다.\n",
    "3. Summa 라이브러리를 사용하여 extractive 요약을 시도하였고, 직접 훈련한 모델을 사용한 abstractive 요약과 결과를 비교하였다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "904ec598f04e298c1da61f59e80a1ebe13847a2b41a30c6e501ecbe66c56a42c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
